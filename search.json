[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "publications",
    "section": "",
    "text": "google scholar\nhttps://orcid.org/0000-0003-3777-3843\nYi, Huimin, Olga Ferlian, Benoit Gauzens, Roberto Rebollo, Stefan Scheu, Angelos Amyntas, Marcel Ciobanu, Anton Potapov, Jörg-Alfred Salamon, and Nico Eisenhauer. 2025. “Belowground Energy Fluxes Determine Tree Diversity Effects on Above- and Belowground Food Webs.” Current Biology, April, S096098222500346X. https://doi.org/10.1016/j.cub.2025.03.034.\nAmyntas, Angelos, Benoit Gauzens, Marcel Ciobanu, Lara Warnke, Mark Maraun, Jörg‐Alfred Salamon, Mona Merkle, et al. 2025. “Shared Community History Strengthens Plant Diversity Effects on Below‐ground Multitrophic Functioning.” Journal of Animal Ecology 94 (4): 555–65. https://doi.org/10.1111/1365-2656.14241.\nHennecke, Justus, Leonardo Bassi, Cynthia Albracht, Angelos Amyntas, Joana Bergmann, Nico Eisenhauer, Aaron Fox, et al. 2024. “Plant Species Richness and the Root Economics Space Drive Soil Fungal Communities.” Ecology Letters 28 (1). https://doi.org/10.1111/ele.70032.\nAmyntas, Angelos, Nico Eisenhauer, Stefan Scheu, Bernhard Klarner, Krassimira Ilieva-Makulec, Anna-Maria Madaj, Benoit Gauzens, et al. 2024. “Soil Community History Strengthens Belowground Multitrophic Functioning Across Plant Diversity Levels in a Grassland Experiment.” Nature Communications 15 (1). https://doi.org/10.1038/s41467-024-54401-z.\nSünnemann, Marie, Andrew D. Barnes, Angelos Amyntas, Marcel Ciobanu, Malte Jochum, Alfred Lochner, Anton M. Potapov, et al. 2024. “Sustainable Land Use Strengthens Microbial and Herbivore Controls in Soil Food Webs in Current and Future Climates.” Global Change Biology 30 (11). https://doi.org/10.1111/gcb.17554.\nLi, Yi, Andreas Schuldt, Anne Ebeling, Nico Eisenhauer, Yuanyuan Huang, Georg Albert, Cynthia Albracht, et al. 2024. “Plant Diversity Enhances Ecosystem Multifunctionality via Multitrophic Diversity.” Nature Ecology & Evolution, August, 1–11. https://doi.org/10.1038/s41559-024-02517-2.\nDyer, Alexander, Remo Ryser, Ulrich Brose, Angelos Amyntas, Nora Bodnar, Thomas Boy, Solveig Franziska Bucher, et al. 2023. “Insect Communities Under Skyglow: Diffuse Night-Time Illuminance Induces Spatio-Temporal Shifts in Movement and Predation.” Philosophical Transactions of the Royal Society B: Biological Sciences 378 (1892). https://doi.org/10.1098/rstb.2022.0359.\nAmyntas, Angelos, Emilio Berti, Benoit Gauzens, Georg Albert, Wentao Yu, Alexandra S. Werner, Nico Eisenhauer, and Ulrich Brose. 2023. “Niche Complementarity Among Plants and Animals Can Alter the Biodiversityecosystem Functioning Relationship.” Functional Ecology 37 (10): 2652–65. https://doi.org/10.1111/1365-2435.14419.\nTerlau, Jördis F., Ulrich Brose, Nico Eisenhauer, Angelos Amyntas, Thomas Boy, Alexander Dyer, Alban Gebler, et al. 2023. “Microhabitat Conditions Remedy Heat Stress Effects on Insect Activity.” Global Change Biology 29 (13): 3747–58. https://doi.org/10.1111/gcb.16712.\nJochum, Malte, Andrew D. Barnes, Ulrich Brose, Benoit Gauzens, Marie Sünnemann, Angelos Amyntas, and Nico Eisenhauer. 2021. “For Flux’s Sake: General Considerations for Energy-Flux Calculations in Ecological Communities.” Ecology and Evolution 11 (19): 12948–69. https://doi.org/10.1002/ece3.8060."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html",
    "href": "posts/20240907-roots-stems-leaves/index.html",
    "title": "Roots, stems & leaves",
    "section": "",
    "text": "Jevon and Lang (2022) examined how tree properties like leaf habit (evergreen vs deciduous) and type of mycorrhizal association (arbuscular vs ectomycorrhizal) can influence the relationship between tree size and biomass allocation, across 55 different tree species. Since this post is going to be light on ecology and heavy on stats, you might want to skim peruse their paper first. It’s really good!\nMy own interest in this data-set is twofold. Examining biomass allocation means that we are looking at proportions; not just the leaf biomass of a tree but the proportion of its total biomass that is leaves. Proportions, which are bound to (0,1), can be analysed using the Beta distribution, so that model estimates respect their bounded nature1. A tree can’t have more than 100% of its biomass in leaves and accordingly, a model using the Beta distribution will not make estimates outside the 0-1 range.\nWhile the Beta distribution allows us to model simple proportions (e.g. proportion of biomass that is leaves vs not leaves), the Dirichlet distribution is an extension of the Beta that can accommodate proportions of more than two components (i.e. proportions of biomass that are leaves, stems and roots)2. On the other hand, working with Dirichlet models is more cumbersome and I am not sure that we get more out of them than fitting three individual Beta models, one for each component. Let’s find out.\nThe other cool feature of Jevon and Lang (2022) is the phylogenetic tree in (their) Figure 1a, showing the relatedness of the 55 species of their study. The authors did use multilevel models to acount for observations clustered within studies and species but they did not make use of the phylogenetic tree in their analysis. We are going to try and use that information to account for the non-independence of related tree species. We are in luck, because {brms} supports this.\nThe authors of the original paper have made all their code available here. They’ve also written a very clear Methods section, outlining every choice they made in the selection of the data, which are coming from the Biomass and Allometry Database3. I have largely retraced their wrangling steps, with minor adjustments4."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#the-point-of-this-exercise",
    "href": "posts/20240907-roots-stems-leaves/index.html#the-point-of-this-exercise",
    "title": "Roots, stems & leaves",
    "section": "",
    "text": "Jevon and Lang (2022) examined how tree properties like leaf habit (evergreen vs deciduous) and type of mycorrhizal association (arbuscular vs ectomycorrhizal) can influence the relationship between tree size and biomass allocation, across 55 different tree species. Since this post is going to be light on ecology and heavy on stats, you might want to skim peruse their paper first. It’s really good!\nMy own interest in this data-set is twofold. Examining biomass allocation means that we are looking at proportions; not just the leaf biomass of a tree but the proportion of its total biomass that is leaves. Proportions, which are bound to (0,1), can be analysed using the Beta distribution, so that model estimates respect their bounded nature1. A tree can’t have more than 100% of its biomass in leaves and accordingly, a model using the Beta distribution will not make estimates outside the 0-1 range.\nWhile the Beta distribution allows us to model simple proportions (e.g. proportion of biomass that is leaves vs not leaves), the Dirichlet distribution is an extension of the Beta that can accommodate proportions of more than two components (i.e. proportions of biomass that are leaves, stems and roots)2. On the other hand, working with Dirichlet models is more cumbersome and I am not sure that we get more out of them than fitting three individual Beta models, one for each component. Let’s find out.\nThe other cool feature of Jevon and Lang (2022) is the phylogenetic tree in (their) Figure 1a, showing the relatedness of the 55 species of their study. The authors did use multilevel models to acount for observations clustered within studies and species but they did not make use of the phylogenetic tree in their analysis. We are going to try and use that information to account for the non-independence of related tree species. We are in luck, because {brms} supports this.\nThe authors of the original paper have made all their code available here. They’ve also written a very clear Methods section, outlining every choice they made in the selection of the data, which are coming from the Biomass and Allometry Database3. I have largely retraced their wrangling steps, with minor adjustments4."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#caveat-lector",
    "href": "posts/20240907-roots-stems-leaves/index.html#caveat-lector",
    "title": "Roots, stems & leaves",
    "section": "caveat lector",
    "text": "caveat lector\nI am not a forest ecologist. I have not spent nearly as much time5 thinking about the processes explored here, as the authors of the original study, nor can I match their expertise on the subject. The motivation behind this is not to question nor affirm the original study. The authors fitted an ecologically motivated model, which they then simplified aiming for parsimony. I am mainly interested in the technicalities of modelling proportions and accounting for phylogenetic relatedness. I will therefore start from the ground up, adding complexity, to see how far I get before I run into trouble. Again, the focus here is on model fitting, not ecology. But I am also, very emphatically, not a statistician. I have tried to explicitly justify every choice I have made and I would like to believe that my choices are sensible. But if you read this and you spot any mistakes, I would be grateful if you point them out to me."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#baby-steps",
    "href": "posts/20240907-roots-stems-leaves/index.html#baby-steps",
    "title": "Roots, stems & leaves",
    "section": "baby steps",
    "text": "baby steps\nif you’d like to play along, the script is available here.\nLet’s start simple, by examining the effect of tree height on root, stem6 and leaf (relative) mass for a single species, Betula alleghaniensis. It’s a good place to start because it is the species with the largest number of individuals in the data (295 trees, unevenly distributed across three locations). Median height is 1.59 m with a range of 0.5-24.42 m. It’s worth keeping in mind that within-species variation in height and biomass allocation does not only depend on ontogeny but also influenced by the environment (e.g. competition with neighbouring trees, climate).\nBefore any model fitting, we are going to standardize our predictor, height; let’s divide the height of each B. alleghaniensis individual by the median height of that species and then take the base 2 logarithm of that quantity. The first of these steps brings individuals of median height at 1. The second step pushes them at 0 (log2(1) = 0) so that the intercepts, the model’s estimates at 0, will be the estimated proportions for a median individual of B. alleghaniensis and now a one-unit change (which is what the coefficient for the predictor corresponds to) amounts to a doubling of height. I’ll do this for all species, not just the focal one.\n\n\nShow the code\nwithin.sp = full_df_mod %&gt;% \n  rename(roots = RmTm,\n         stems = SmTm,\n         leaves = LmTm) %&gt;% \n  # this is how brms expects the response for Dirichlet models \n  mutate(RSL = cbind(roots = .$roots,\n                     stems = .$stems,\n                     leaves = .$leaves)) %&gt;% \n  group_by(SppName) %&gt;% \n  mutate(.after = h.t,\n         ht.median = median(h.t),\n         rel.ht = log2(h.t/ht.median),\n         N = n()) %&gt;% \n  ungroup()\n\n\nNow we will first fit three Beta regression models, each one estimating the effect of relative height on the proportion of an individual’s total biomass that is roots, stems or leaves. We will let the intercept vary by study. Then we will estimate the same relationships but this time, fitting one model for all three proportions7.\n\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\n# one model for roots\nm.Bet_alle.r &lt;- brm(bf(roots ~ rel.ht + (1|Study)), \n                    data = within.sp %&gt;% \n                      filter(SppName == \"Betula_alleghaniensis\"), \n                    family = Beta(), \n                    chains = 4, \n                    iter = 6000, \n                    warmup = 3000, \n                    cores = 4, \n                    control = list(adapt_delta = 0.99),\n                    backend = \"cmdstanr\",\n                    file = \"fits/m.Bet_alle.r\",\n                    seed = 123)\npp_check(m.Bet_alle.r, ndraws = 200)\n\n# one model for stems\nm.Bet_alle.s &lt;- brm(bf(stems ~ rel.ht + (1|Study)), \n                    data = within.sp %&gt;% \n                      filter(SppName == \"Betula_alleghaniensis\"), \n                    family = Beta(), \n                    chains = 4, \n                    iter = 6000, \n                    warmup = 3000, \n                    cores = 4, \n                    control = list(adapt_delta = 0.99,\n                                   max_treedepth = 12),\n                    backend = \"cmdstanr\",\n                    file = \"fits/m.Bet_alle.s\",\n                    seed = 123)\npp_check(m.Bet_alle.s, ndraws = 200)\n\n# one model for leaves\nm.Bet_alle.l &lt;- brm(bf(leaves ~ rel.ht + (1|Study)), \n                    data = within.sp %&gt;% \n                      filter(SppName == \"Betula_alleghaniensis\"), \n                    family = Beta(), \n                    chains = 4, \n                    iter = 6000, \n                    warmup = 3000, \n                    cores = 4, \n                    control = list(adapt_delta = 0.99,\n                                   max_treedepth = 12),\n                    backend = \"cmdstanr\",\n                    file = \"fits/m.Bet_alle.l\",\n                    seed = 123)\npp_check(m.Bet_alle.l, ndraws = 200)\n\n\n# all in one\nm.Bet_alle.rsl &lt;- brm(bf(RSL ~ rel.ht + (1|Study)), \n                      data = within.sp %&gt;% \n                        filter(SppName == \"Betula_alleghaniensis\"), \n                      family = dirichlet(), \n                      chains = 4, \n                      iter = 9000, \n                      warmup = 3000, \n                      cores = 4, \n                      control = list(adapt_delta = 0.99,\n                                     max_treedepth = 12),\n                      backend = \"cmdstanr\",\n                      file = \"fits/m.Bet_alle.rsl\",\n                      seed = 123)\n# Total execution time: 751.3 seconds.\npp_check(m.Bet_alle.rsl, ndraws = 100)\n# Error: 'pp_check' is not implemented for this family.\n\nFirst sign of “trouble”; we cannot use pp_check() for a Dirichlet regression, to assess how well the model captures the distribution of the data. We can make density overlay plots from scratch though:\n\nlibrary(bayesplot)\n# generate predictions\nyrep &lt;- posterior_predict(m.Bet_alle.rsl, ndraws = 200)\n# yrep is a three dimensional array:\n# 200 predictions x 295 observations x 3 response components\ndim(yrep)\n# here's one slice:\nyrep[1, 1, 1:3]\n#     roots     stems    leaves \n# 0.3791285 0.4160540 0.2048174\n\n# three custom-made density overlay plots, similar to the default pp_check() output\nppc_dens_overlay(within.sp$roots[within.sp$SppName == \"Betula_alleghaniensis\"], \n                 yrep[ , 1:295, 1])\nppc_dens_overlay(within.sp$stems[within.sp$SppName == \"Betula_alleghaniensis\"], \n                 yrep[ , 1:295, 2])\nppc_dens_overlay(within.sp$leaves[within.sp$SppName == \"Betula_alleghaniensis\"], \n                 yrep[ , 1:295, 3])\n\nAnd now we can compare the estimates of the two approaches. To make it easier to see any differences, I have combined the draws from the three Beta models before pushing them through {ggplot}, so that we can have one plot for the three Betas (left) and one for the Dirichlet regression (right):\n\n\nShow the code\nroot = within.sp %&gt;%\n  data_grid(rel.ht = seq_range(rel.ht, n = 51)) %&gt;%\n  add_epred_draws(m.Bet_alle.r, re_formula = NA) %&gt;% \n  add_column(.category = \"roots\")\n\nstem = within.sp %&gt;%\n  data_grid(rel.ht = seq_range(rel.ht, n = 51)) %&gt;%\n  add_epred_draws(m.Bet_alle.s, re_formula = NA) %&gt;% \n  add_column(.category = \"stems\")\n\nleaf = within.sp %&gt;%\n  data_grid(rel.ht = seq_range(rel.ht, n = 51)) %&gt;%\n  add_epred_draws(m.Bet_alle.l, re_formula = NA) %&gt;% \n  add_column(.category = \"leaves\")\n\nrsl = rbind(root,stem,leaf)\nrsl$.category = factor(rsl$.category, levels = c(\"roots\",\"stems\",\"leaves\"))\n\nfig1a = rsl %&gt;%\n  ggplot(aes(x = rel.ht, y = .epred, color = .category,\n             fill = .category)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey\") +\n  stat_lineribbon(aes(y = .epred), .width = c(0.9),\n                  point_interval = \"median_qi\",\n                  linewidth = .2)+\n  scale_x_continuous(breaks = c(-3,-1,0,1,3), \n                     labels = c(\"1/8\",\"1/2\",1,2,8)) +\n  scale_color_manual(values = c(\"#efd08e\",\"#8d8067\",\"#9dbc9b\")) +\n  scale_fill_manual(values = c(\"#efd08e80\",\"#8d806780\",\"#9dbc9b80\")) +\n  ylab(\"Biomass proportion\") +\n  xlab(\"multiplicative change in relative height\") +\n  theme_bw() +\n  theme(legend.position = \"none\") + \n  ggtitle(\"Predictions of three Beta regressions\")\n\nfig1b = within.sp %&gt;%\n  data_grid(rel.ht = seq_range(rel.ht, n = 51)) %&gt;%\n  add_epred_draws(m.Bet_alle.rsl, re_formula = NA) %&gt;%\n  ggplot(aes(x = rel.ht, y = .epred, color = .category,\n             fill = .category)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey\") +\n  stat_lineribbon(aes(y = .epred), .width = c(0.9),\n                  point_interval = \"median_qi\",\n                  linewidth = .2)+\n  scale_x_continuous(breaks = c(-3,-1,0,1,3), \n                     labels = c(\"1/8\",\"1/2\",1,2,8)) +\n  scale_color_manual(values = c(\"#efd08e\",\"#8d8067\",\"#9dbc9b\")) +\n  scale_fill_manual(values = c(\"#efd08e80\",\"#8d806780\",\"#9dbc9b80\")) +\n  ylab(\"Biomass proportion\") +\n  xlab(\"multiplicative change in relative height\") +\n  theme_bw() +\n  theme(legend.title = element_blank()) + \n  ggtitle(\"Predictions of Dirichlet regression\")\n\nlibrary(patchwork)\nfig1 = fig1a+fig1b\n\nggsave(\"figs/fig1.png\", fig1, units = \"mm\",\n       width = 180,\n       height = 75,\n       scale = 1.4)\n\n\n\n\n\nFigure 1: Model estimates with the two different approaches. Thin lines are the median estimates, bound by 90% credible envelopes.\n\n\nThey are not alarmingly different but you can see the wider uncertainty for the stem proportion across the height gradient and the change in the shape of the root-height relationship. The big picture is the same though and at this point, while we are only looking at one species and not yet asking how leaf habit or mycorrhizal association can modify this relationship, these differences are inconsequential.\nSo taller trees allocate increasingly more biomass to stems, relative to roots or leaves. An individual of median height is already around 50% stem, while leaves are the smallest proportion. (Probably not news to anyone studying tree allometry; I wouldn’t know)."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#all-together-now",
    "href": "posts/20240907-roots-stems-leaves/index.html#all-together-now",
    "title": "Roots, stems & leaves",
    "section": "all together now",
    "text": "all together now\nGetting cocky, we are now going to throw the full dataset into the Dirichlet model. We now have 55 species with different leaf habits and mycorrhizal associations. Biomass allocation, as well as its relationship to height, may be influenced by these factors. I will cut a few corners here8, going directly to a model that includes relative height, habit and mycorrhizal group as well as their two-way interactions as predictors. We have a fairly sparse dataset with species information coming from more than one study/location while studies/locations feature more than one species. This sounds like a case for crossed random effects; we will account for the clustering of observations within species and within studies. We are also going to let the effect of height vary by species. Remember that we are still looking at within species height variation, with species’ median height as our baseline. We’re just partially pooling information across species about the effects of within-species height variation on biomass allocation.\n\nm.all.rsl_hhm2 &lt;- brm(bf(RSL ~ (rel.ht  + leaf_habit + myc_group)^2 + \n                           (1 + rel.ht|SppName) + (1|Study)), \n                      data = within.sp, \n                      family = dirichlet(), \n                      chains = 4, \n                      iter = 9000, \n                      warmup = 3000, \n                      cores = 4, \n                      control = list(adapt_delta = 0.99,\n                                     max_treedepth = 12),\n                      backend = \"cmdstanr\",\n                      file = \"fits/m.all.rsl_hhm2\",\n                      seed = 123)\n# Total execution time: 4191.0 seconds.\n\nWe have now ramped up both the number of observations and, more importantly, the number of coefficients to be estimated. This model took a while to get there.\n\n\n\nFigure 2: Estimates of the Dirichlet model for all species, faceted by leaf habit and mycorrhizal association. Thin lines are the median estimates, bound by 90% credible envelopes.\n\n\nWhat stood out to me first, by eyeballing figure 2, is that ECM-associated trees seem to have a more pronounced stem to leaf biomass trade-off, especially above median height.\nHere are the model estimates at median height for the two leaf habits and mycorrhizal associations.\n\n\nShow the code\nfig3a = within.sp %&gt;% \n  data_grid(expand_grid(rel.ht = c(0),\n                        leaf_habit = c(\"deciduous\"),\n                        myc_group = c(\"ECM\",\"AM\"))) %&gt;% \n  add_epred_draws(m.all.rsl_hhm2,\n                  re_formula = NA) %&gt;% \n  mutate(group = str_c(.category, myc_group)) %&gt;% \n  ggplot(aes(x = .epred, fill = factor(group))) +\n  stat_halfeye(aes(shape = myc_group),\n               .width = c(.9),\n               size = 5,\n               alpha = .5) +\n  scale_fill_manual(values = c(\"#9dbc9b\",\"#9dbc9b\",\n                               \"#efd08e\",\"#efd08e\",\n                               \"#8d8067\",\"#8d8067\"),\n                    guide = \"none\") +\n  ylab(\"\") +\n  xlab(\"estimate\") +\n  facet_wrap(~.category,\n             scales = \"free\") +\n  theme_bw() + \n  ggtitle(\"Biomass allocation of deciduous trees at median height\")\n\nfig3b = within.sp %&gt;% \n  data_grid(expand_grid(rel.ht = c(0),\n                        leaf_habit = c(\"evergreen\"),\n                        myc_group = c(\"ECM\",\"AM\"))) %&gt;% \n  add_epred_draws(m.all.rsl_hhm2,\n                  re_formula = NA) %&gt;% \n  mutate(group = str_c(.category, myc_group)) %&gt;% \n  ggplot(aes(x = .epred, fill = factor(group))) +\n  stat_halfeye(aes(shape = myc_group),\n               .width = c(.9),\n               size = 5,\n               alpha = .5) +\n  scale_fill_manual(values = c(\"#9dbc9b\",\"#9dbc9b\",\n                               \"#efd08e\",\"#efd08e\",\n                               \"#8d8067\",\"#8d8067\"),\n                    guide = \"none\") +\n  ylab(\"\") +\n  xlab(\"estimate\") +\n  facet_wrap(~.category,\n             scales = \"free\") +\n  theme_bw() + \n  ggtitle(\"Biomass allocation of evergreen trees at median height\")\n\n\nfig3 = fig3a/fig3b\nggsave(\"figs/fig3.png\", fig3, units = \"mm\",\n       width = 180,\n       height = 120,\n       scale = 1.4)\n\n\n\n\n\nFigure 3: Estimated proportions at median height. Points are the median estimate bound by 90% credible intervals. Note that each facet has its own scale for the x axis. The y axis is not informative; I forgot to remove it."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#the-other-kind-of-tree",
    "href": "posts/20240907-roots-stems-leaves/index.html#the-other-kind-of-tree",
    "title": "Roots, stems & leaves",
    "section": "the other kind of tree",
    "text": "the other kind of tree\nFinally, we can take our already complex multivariate, multivariable , varying intercepts, varying slopes regression model and have the intercepts and height slopes further constrained by phylogeny. This means that when we are partially pooling information across species, the extent of pooling depends on how closely related the different species are. You can read more about phylogenetic multilevel models with {brms} here. Getting the data set-up right was a process of trial and error, but {brms} throws fairly informative errors. We need:\n\na phylogenetic tree (provided by Jevon and Lang (2022)) to construct the variance-covariance matrix,\na grouping variable (phylo) telling us which node of the tree each observation belongs to, but this needs to be distinct from our species variable (SppName) even though they (must) contain the same information,\nthe names of the vcv matrix need to match the elements of phylo.\n\nNote that we are dealing with a phylogenetic model with repeated measurements within each node/species which is why we need to go through the SppName-phylo matching shenanigans. We’ve also had not just the intercept but also the effect of height on biomass allocation be constrained by phylogeny. This model took a substantial amount of time to finish.\n\nlibrary(ape)\n# load the tree\nphylo = read.tree(\"phyliptree1.phy\")\n# phylogenetic variance-covariance matrix\nA = ape::vcv.phylo(phylo)\n# replaces \"name\" with name\ndimnames(A) = list(unique(within.sp$SppName),\n                   unique(within.sp$SppName))\n# grouping of observations by species (brms requires a distinct named variable)\nwithin.sp$phylo = within.sp$SppName\n\nm.all.rsl_hhm2_h.ph &lt;- brm(bf(RSL ~ (rel.ht  + leaf_habit + myc_group)^2 + \n                                (1 + rel.ht|gr(phylo, cov = A)) + (1 + rel.ht|SppName) + (1|Study)), \n                           data = within.sp, \n                           data2 = list(A = A),\n                           family = dirichlet(), \n                           chains = 4, \n                           iter = 9000, \n                           warmup = 3000, \n                           cores = 4, \n                           control = list(adapt_delta = 0.99,\n                                          max_treedepth = 12),\n                           backend = \"cmdstanr\",\n                           file = \"fits/m.all.rsl_hhm2_h.ph\",\n                           seed = 123)\n\nAnd here are the estimates of the phylogenetic model:\n\n\n\nFigure 4: Estimates of the phylogenetic model. Thin lines are the median estimates, bound by 90% credible envelopes.\n\n\nIf you compare it with the previous figure, the credible envelopes are chonkier. That’s as it should be; the information we get from each species is not independent from other, related species.\nFor completeness, here are the estimates of the three-Betas equivalent of the phylogenetic model.\nAnd finally, inspired by this amazing plot, let’s make our own version of figure 2d in Jevon and Lang (2022). We will stick with median height:\n\n\nShow the code\nlibrary(scales)\n\nfig5 = within.sp %&gt;%\n  data_grid(rel.ht = c(0),\n            leaf_habit = c(\"deciduous\",\n                           \"evergreen\"),\n            myc_group = c(\"ECM\",\"AM\")) %&gt;%\n  add_epred_draws(m.all.rsl_hhm2_h.ph, re_formula = NA) %&gt;% \n  mutate(.category=fct_relevel(.category,c(\"leaves\",\"stems\",\"roots\"))) %&gt;% \n  ggplot(aes(x = .draw, y = .epred)) +\n  geom_area(aes(fill = .category), position = position_stack()) +\n  labs(x = NULL, y = \"Proportion of total biomass\", fill = NULL) +\n  scale_x_continuous(breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(labels = label_percent(), expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#9dbc9bcc\",\"#8d8067cc\",\"#efd08ecc\")) +\n  facet_wrap(leaf_habit~myc_group, strip.position = \"bottom\", nrow = 1) +\n  theme_bw()\n\nggsave(\"figs/fig5.png\", fig5, units = \"mm\",\n       width = 140,\n       height = 140,\n       scale = 1)\n\n\n\n\n\nFigure 5: Biomass allocation for an individual of median height. The fuzzy areas demonstrate the uncertainty for the expected proportions"
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#conclusions",
    "href": "posts/20240907-roots-stems-leaves/index.html#conclusions",
    "title": "Roots, stems & leaves",
    "section": "conclusions",
    "text": "conclusions\nA Dirichlet regression is three \\(n\\) Beta regressions in a trenchcoat 🤷.\nWorking with the output of a Dirichlet model, to produce figures or to get estimates at specific values, was more straightforward than I anticipated. As far as I can tell, it is not possible to use {marginaleffects} or {emmeans} to calculate contrasts, for example. But these can be calculated manually from the posterior draws (left as an exercise for the reader 😉).\nThe phylogenetic model is simple enough to specify but can be computationally prohibitive depending on your processor and RAM.\nTrees are interesting 🤔."
  },
  {
    "objectID": "posts/20240907-roots-stems-leaves/index.html#footnotes",
    "href": "posts/20240907-roots-stems-leaves/index.html#footnotes",
    "title": "Roots, stems & leaves",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t need to go into that because there an excellent discussion here↩︎\nA few years back Douma and Weedon (2019) wrote a review on how to work with continuous proportions with these two distributions. Their overview of support for proportional models is only slightly out of date; brms does support Dirichlet models.↩︎\n(Falster et al. 2015)↩︎\nThe most important of them was extracting temperature and precipitation data from Worldclim 2.1 instead of 1.4. The latter is no longer accessible1.\n4.1: (a meta-footnote 😎) You’d think this is a breaking change but in their repository, Jevon and Lang include the bioclimatic data necessary to run their code without interruptions. So, from a strict reproducibility perspective, ⭐⭐⭐⭐⭐↩︎\nThis is my twisted idea of a Sunday mornings’ leisure activity.↩︎\nI was very invested in a shoots and leaves joke but\n\nthe roots spoil it\nturns out stems and shoots are not interchangeable. Apparently, any ecologist should know that.\n\n↩︎\nTechnically, were estimating two of them, getting the third one for free, which is why the Dirichlect model has n-1 of everything (intercepts, slope coefficients), for n components. This is similar to how in a Beta regression, we are implicitly also estimating the proportion that is not leaves, for example.↩︎\nThere were no convergence problems with models that included no interactions and intercept-only random effects.↩︎"
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html",
    "href": "posts/2025____-shorttitle/index.html",
    "title": "Eyeballing it: working with censored data",
    "section": "",
    "text": "When we measure the density of phytoplankton per volume of water or the body-mass of ground beetle individuals, our observations take specific values (45 cells in the sample volume, 45mg for this beetle). These observations have sampling error (a different sample from the same observational unit would contain more or less than 45 cells) and measurement error (the scale we use to measure beetle mass has precision limits) and this error is often unknown.\nNow consider the flip side of this: observations that are inherently imprecise because all we know is that they fall within a certain range or beyond a limit of measurement. In this case, we know only the bounds of measurement error but we do not have a specific value to assign on each observation. This is censoring.\nIf an observation falls below the threshold of what a device can measure, the device will show zero. In practice, this zero stands for “less than the minimum” and is indistinguishable from a true zero (when the thing you are measuring is really absent). This is left censoring. When we get the largest value a device can measure, we have right censoring.\nInterval censoring is perhaps the most intuitive case; we know the true value falls within a specific range – say more than 5 and up to 10 i.e. (5, 10]. Interval censoring can be a feature rather than a bug; in my broad neighbourhood of ecology, the typical example would be protocols designed for vegetation cover surveys, such as the Daubenmire scale (Daubenmire 1959). Under such a measurement scheme, observations are assigned in one of several distinct classes: 0%, (0, 5]%, (5, 25]%, (25, 50]% and so on, of ground covered by vegetation. The exact intervals will vary from case to case. Collecting data in this approximate fashion can be efficient and a standardized scheme ensures consistency among different surveyors.\nIn this post, I will use as an example case a study (Ebeling et al., 2021) that used a similar scheme to quantify leaf damage from herbivory by assigning each observation (the proportion of damage on an individual leaf) to one of four categories 0%, (0, 5]%, (5, 25]%, (25, 100)%. Notice that the scale starts with a precise zero (absence of damage) followed by increasingly large intervals. Even leaves of the same species will vary in size and exact shape; the more of a leaf that has been eaten away, the harder it is to specify the exact proportion of damage.\nThe question is then, how to best analyze data that have been collected in such a way? We are going to consider some of the available options. Let’s dive in!"
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#what-the",
    "href": "posts/2025____-shorttitle/index.html#what-the",
    "title": "Eyeballing it: working with censored data",
    "section": "",
    "text": "When we measure the density of phytoplankton per volume of water or the body-mass of ground beetle individuals, our observations take specific values (45 cells in the sample volume, 45mg for this beetle). These observations have sampling error (a different sample from the same observational unit would contain more or less than 45 cells) and measurement error (the scale we use to measure beetle mass has precision limits) and this error is often unknown.\nNow consider the flip side of this: observations that are inherently imprecise because all we know is that they fall within a certain range or beyond a limit of measurement. In this case, we know only the bounds of measurement error but we do not have a specific value to assign on each observation. This is censoring.\nIf an observation falls below the threshold of what a device can measure, the device will show zero. In practice, this zero stands for “less than the minimum” and is indistinguishable from a true zero (when the thing you are measuring is really absent). This is left censoring. When we get the largest value a device can measure, we have right censoring.\nInterval censoring is perhaps the most intuitive case; we know the true value falls within a specific range – say more than 5 and up to 10 i.e. (5, 10]. Interval censoring can be a feature rather than a bug; in my broad neighbourhood of ecology, the typical example would be protocols designed for vegetation cover surveys, such as the Daubenmire scale (Daubenmire 1959). Under such a measurement scheme, observations are assigned in one of several distinct classes: 0%, (0, 5]%, (5, 25]%, (25, 50]% and so on, of ground covered by vegetation. The exact intervals will vary from case to case. Collecting data in this approximate fashion can be efficient and a standardized scheme ensures consistency among different surveyors.\nIn this post, I will use as an example case a study (Ebeling et al., 2021) that used a similar scheme to quantify leaf damage from herbivory by assigning each observation (the proportion of damage on an individual leaf) to one of four categories 0%, (0, 5]%, (5, 25]%, (25, 100)%. Notice that the scale starts with a precise zero (absence of damage) followed by increasingly large intervals. Even leaves of the same species will vary in size and exact shape; the more of a leaf that has been eaten away, the harder it is to specify the exact proportion of damage.\nThe question is then, how to best analyze data that have been collected in such a way? We are going to consider some of the available options. Let’s dive in!"
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#the-study",
    "href": "posts/2025____-shorttitle/index.html#the-study",
    "title": "Eyeballing it: working with censored data",
    "section": "the study",
    "text": "the study\nEbeling et al 2021 studied the effect that nutrient addition can have on herbivory1 damage to grassland plants. They did so in plots of the Nutrient Network (REF) a globally distributed experiment of nutrient addition effects on grassland plant diversity and productivity. The study is open access, so you go can dig into it for details. It’s a really nice study! Here, I will provide the bare necessities: each location has control plots and treatment plots that have received either Nitrogen or Phosphorus or both. Leaf damage from herbivores was evaluated on one fully expanded leaf from five individuals per species per functional group (that is, grasses, forbs and legumes). As already mentioned, each leaf was assigned to one of four categories: 0%, (0, 5]%, (5, 25]% or (25, 100)% damage. The study covers more ground than that but here I will focus on a specific analysis. So the empirical question is, does nutrient addition have an influence on herbivory damage and does that influence vary among functional groups? The practical question which is the topic of this post, is what kind of model do we need to answer that empirical question."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#caveat-lector",
    "href": "posts/2025____-shorttitle/index.html#caveat-lector",
    "title": "Eyeballing it: working with censored data",
    "section": "caveat lector",
    "text": "caveat lector\nI am a stats enthusiast – I enjoy thinking about and doing statistics – but I am not a statistician. Occasionally, while reading a paper in my field, I think hang on, I believe there may be a better way to do this analysis. Posts like this are the product of that thought process. My main motivation is curiosity and the opportunity to sharpen my own skills. This is largely self-study but if you read along and find something useful, I am glad it helped. If you do spot my errors or something that does’t quite make sense, I would appreciate the feedback."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#yolo",
    "href": "posts/2025____-shorttitle/index.html#yolo",
    "title": "Eyeballing it: working with censored data",
    "section": "yolo",
    "text": "yolo\nGood ol’ linear regression is notoriously2 robust to having its assumptions violated. Arguably the chaotic neutral option here, is to rely on this legendary robustness and keep it simple. Which as it happens, is what the authors of the original study did: they used the midpoint value of each range to represent the range of each observation. They then log(x+1) transformed those values and fitted a model with a Gaussian distribution. Clutch your pearls if you must; I’m going to spoil this post right here by saying that this model leads to more or less the same conclusions as anything fancier we’ll try later on.\nWe begin by (more or less) retracing their steps.\n\nlibrary(tidyverse)\nlibrary(brms)     \nlibrary(cmdstanr) \nlibrary(tidybayes)\nlibrary(modelr)\n\n# data from Ebeling et al. 2021 https://doi.org/10.1111/1365-2745.13801\n# retrieved from https://datadryad.org/dataset/doi:10.5061/dryad.gf1vhhmqs\ndata = read_delim(\"data_LeafDamage_NutNet.csv\", delim = \";\") %&gt;% \n  select(site, plot, experimental_treatment, \n         functional_group, taxon, taxon_replicate.plot,\n         invert_damage.perc) %&gt;% \n  rename(treat = experimental_treatment,\n         fgroup = functional_group) %&gt;% \n  filter(fgroup != \"WOODY\") %&gt;% \n  mutate(fgroup = factor(fgroup,\n                         levels = c(\"GRASS\",\"FORB\",\"LEGUME\")),\n         # just making sure observations conform to the midpoint rule\n         invert_damage.perc = case_when(invert_damage.perc ==  0 ~ 0,\n                                        invert_damage.perc  &lt;  6 ~ 3,\n                                        invert_damage.perc  &lt; 26 ~ 15,\n                                        invert_damage.perc &gt;= 26 ~ 63),\n         logdam = log(invert_damage.perc + 1))\n\nOur model has two categorical predictors3, treatment and functional group, as well as their interaction as main effects. Then intercepts4 vary by site, taxon, and taxon within plot. Aside from the changed treatment coding, this model is the Bayesian equivalent to the one reported in Table S5 in Ebeling et al. (2021).\n\nS5.1 = brm(\n  data = data,\n  bf(\n    logdam ~ treat + fgroup + treat:fgroup +\n      (1 | site) + \n      (1 | taxon) + \n      (1 | site:plot:taxon)\n  ),\n  family = gaussian(),\n  iter = 6000,\n  warmup = 2000,\n  chains = 4, \n  cores = 4, \n  seed = 123,\n  backend = \"cmdstanr\"\n)\n\nHere are the estimates for the average (log-transformed) damage per group and treatment:\n\n\nShow the code\np1 = data %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;%  \n  add_epred_draws(S5.1, re_formula = NA) %&gt;%\n  ggplot(aes(x = treat, y = .epred, fill = treat)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95)) +\n  scale_fill_manual(values = c(\"#efd08e\",\"#afbedc\",\"#a33c20\", \"#702963\")) +\n  ylab(\"log(damage + 1)\") +\n  xlab(\"nutrient addition treatment\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_grid(. ~ fgroup)\n\np2 = data %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;%  \n  add_epred_draws(S5.1, re_formula = NA) %&gt;%\n  compare_levels(.epred, by = treat,\n                 comparison = control) %&gt;%\n  ggplot(aes(x = .epred, y = treat)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95)) +\n  geom_vline(xintercept = 0, linetype = \"longdash\",\n             color = \"darkred\") +\n  ylab(\"contrast\") +\n  xlab(\"difference\") +\n  facet_wrap(~fgroup) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: Posterior estimates by group and treatment. Mean estimates are bound by a 90% and a 95% HPD interval.\n\n\nIt looks like legumes tend to be preferred by (invertebrate) herbivores but there is also more uncertainty for their average level of damage. And here are the estimates for the average differences of the three treatments to the control:\n\n\n\nFigure 2: Posterior estimates for the difference between treatments and control by group.\n\n\nWe see clear evidence of increased herbivory in both treatments with Nitrogen addition (N and NP) for grasses and forbs. Treatment plots with Phosphorus addition also show increased herbivory in grasses and forbs relative to controls but that difference is weaker.\nBut let’s do a posterior predictive check, to see how well our model reproduces the distribution of the data:\n\npp_check(S5.1, ndraws = 100)\n\n\n\n\nFigure 3: Density overlay posterior predictive check. The dark blue bold line shows the distribution of the data. The light blue thin lines are 100 predictive distributions.\n\n\nOuch. The four spikes are the log-transformed midpoint values that our observations can take. They look like densities because density overlay is the default option in pp_check() but we just have four distinct values; a histogram would be more appropriate for these data. On the other hand, the density overlay is an appropriate choice for the kind of model we fitted. The real discrepancy is between the model and the data. Anyway, this is a good-enough first diagnostic. If the light blue lines were properly overlaid on the dark blue line, we would be reassured. Unsurprisingly, our model cannot handle that. We are going to see whether we can do better.\nBy the way, a reasonable modification here would be to acknowledge that we are dealing with proportions and use a (zero inflated) Beta distribution. But this will not do any good before we move away from reducing our interval data to midpoint values."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#ordinal-regression",
    "href": "posts/2025____-shorttitle/index.html#ordinal-regression",
    "title": "Eyeballing it: working with censored data",
    "section": "ordinal regression",
    "text": "ordinal regression\nWe are dealing with semi-quantitative proportional data that are intervals rather than precise values. However, before we try introducing the interval censoring explicitly into our model, I want to move in a different direction.\nI will first lean into the semi-quantitativeness of things and treat the data as ordinal. We have four categories that represent no, little, some or extensive damage. I did not invest a lot of time coming up with these names; they are not important. What is important is that every level represents more damage than the previous one. A cumulative probit model, which is what we are trying next, that an ordinal response with n levels is partitioning a latent continuous variable using n - 1 thresholds. These thresholds are like intercepts in our model and their location along the latent scale is dependent on our predictors.\nLuckily for me and you, there is an excellent introduction5 to cumulative probit models out there that is both comprehensive and accessible (h/t to Solomon Kurz). This means I can refer you to that for a deeper understanding of what these models do and how they do it. Here we are just going to use such a model. This is an intermediate stop (a detour really) to where this post is going.\nFor a cumulative model, {brms} accepts data in two possible types: either as integers (representing ordinal levels) or as ordered factors. I will go with the second option. Keep in mind that if you name your factor levels they will need to be explicitly ordered correctly or they will be ordered alphabetically:\n\ndataord = data %&gt;% \n  mutate(# for the ordinal regression\n         damord = case_when(invert_damage.perc  ==  0 ~ \"1\",\n                               invert_damage.perc  &lt;   6 ~ \"2\",\n                               invert_damage.perc  &lt;  26 ~ \"3\",\n                               invert_damage.perc &gt;=  26 ~ \"4\") %&gt;% \n           as.ordered()\n         )\n\nTwo thing are different here. We have swapped the Gaussian distribution for the cumulative probit. We have also introduced a new relationship to be estimated, disc ~ fgroup.\n\nS5.1_ord = brm(\n  data = dataord,\n  bf(\n    damord ~ treat + fgroup + treat:fgroup +\n      (1 | site) + \n      (1 | taxon) + \n      (1 | site:plot:taxon),\n    disc ~ fgroup\n  ),\n  family = cumulative(\"probit\"),\n  iter = 6000,\n  warmup = 2000,\n  control = list(adapt_delta = .95),\n  chains = 4, \n  cores = 4, \n  seed = 42,\n  backend = \"cmdstanr\"\n)\n\nLet’s start with the posterior predictive check:\n\n\n\nFigure 4: Density overlay posterior predictive check. The dark blue bold line shows the distribution of the data. The light blue thin lines are 100 predictive distributions.\n\n\nAgain, this is not the most appropriate posterior predictive check we could do for this type of data but it is directly comparable to the previous one. You can see that the estimated “distribution” follows the spikes of the data.\nThis model is also a good example where scrutinizing individual regression coefficients to make inference is not very helpful; everything is on the latent scale. We want to understand the model estimates in terms of the ordinal levels we have used for our response.\n\n Family: cumulative \n  Links: mu = probit; disc = log \nFormula: damord ~ treat + fgroup + treat:fgroup + (1 | site) + (1 | taxon) + (1 | site:plot:taxon) \n         disc ~ fgroup\n   Data: dataord (Number of observations: 10491) \n  Draws: 4 chains, each with iter = 6000; warmup = 2000; thin = 1;\n         total post-warmup draws = 16000\n\nMultilevel Hyperparameters:\n~site (Number of levels: 27) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     7.68      3.26     3.36    15.99 1.00     1743     2261\n\n~site:plot:taxon (Number of levels: 2047) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     4.25      1.69     1.97     8.57 1.00     1600     2231\n\n~taxon (Number of levels: 153) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     6.65      2.67     3.04    13.52 1.00     1588     2130\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]             3.25      1.97     0.02     7.80 1.00     2363     3265\nIntercept[2]            14.95      5.89     6.79    29.71 1.00     1605     2216\nIntercept[3]            25.23      9.86    11.70    49.98 1.00     1578     2174\ndisc_Intercept          -2.04      0.37    -2.81    -1.33 1.00     1575     2162\ntreatN                   2.31      1.15     0.76     5.14 1.00     2112     2464\ntreatNP                  1.55      0.92     0.24     3.84 1.00     2928     2509\ntreatP                   0.87      0.78    -0.41     2.75 1.00     4596     3582\nfgroupFORB               0.94      1.53    -1.86     4.27 1.00     3186     4068\nfgroupLEGUME             6.92      3.86     1.46    16.49 1.00     2218     2685\ntreatN:fgroupFORB        0.26      0.94    -1.58     2.26 1.00     7560     6008\ntreatNP:fgroupFORB       1.93      1.20     0.20     4.87 1.00     3592     4306\ntreatP:fgroupFORB        0.51      0.97    -1.29     2.64 1.00     7362     6170\ntreatN:fgroupLEGUME     -3.86      2.54   -10.14    -0.20 1.00     3041     2756\ntreatNP:fgroupLEGUME    -2.42      2.22    -7.67     1.20 1.00     4501     3793\ntreatP:fgroupLEGUME     -1.38      2.01    -5.94     2.19 1.00     6487     4223\ndisc_fgroupFORB         -0.09      0.03    -0.14    -0.03 1.00    17974    12628\ndisc_fgroupLEGUME       -0.27      0.05    -0.38    -0.16 1.00    18444    12974\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nOne way we can plot model estimates in the ordinal scale is the following. The y-axis is the relative probability of the four different levels. For example, looking at grasses across treatments, we see that the probability of what we called severe damage (level 4) is zero. No damage (level 1) has the highest probability followed by little damage (2). Conversely, legumes are the group with the highest probability of levels 3 and 4 among the three functional groups.\n\n\nShow the code\np3 = dataord %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;%  \n  add_epred_draws(S5.1_ord, re_formula = NA) %&gt;%\n  ggplot(aes(x = .category, y = .epred)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95)) +\n  ylab(\"probability\") +\n  xlab(\"damage level\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  facet_grid(vars(fgroup), vars(treat))\n\n\n\n\n\nFigure 5: Posterior estimates for the relative probability of each damage level by treatment and group.\n\n\nThis way of looking at estimates is not very conducive to making comparisons. To do that, we can estimate the average level of damage for each group and treatment, treating our levels as integers, weighting them by their relative probabilities and calculating a weighted average. An average level of 1.5 would indicate damage between no and little. We do so per draw from the posterior. The we can plot those estimates:\n\np4 = dataord %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;%  \n  add_epred_draws(S5.1_ord, re_formula = NA) %&gt;%\n  group_by(treat, fgroup, .draw) %&gt;%\n            # calculate weighted average per draw \n  summarise(damord = weighted.mean(as.numeric(.category), .epred)) %&gt;%\n  ggplot(aes(x = treat, y = damord, fill = treat)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95)) +\n  scale_fill_manual(values = c(\"#efd08e\",\"#afbedc\",\"#a33c20\", \"#702963\")) +\n  ylab(\"average damage level\") +\n  xlab(\"nutrient addition treatment\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_grid(. ~ fgroup)\n\n\n\n\nFigure 6: Posterior estimates by group and treatment for the cumulative probit model.\n\n\nAnd similarly we can plot the estimates for average differences:\n\n\n\nFigure 7: Posterior estimates for the difference between treatments and control by group for the cumulative probit model.\n\n\nEven though the two models are very different in terms of their specification, the conclusions we would draw from the estimates of this model are qualitatively similar to those from the first one. In a couple of cases, the 95% credible intervals for the estimated differences are now straddling zero when in the first model they did not, but the broad picture is the same."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#zero-inflated-censored-beta-regression",
    "href": "posts/2025____-shorttitle/index.html#zero-inflated-censored-beta-regression",
    "title": "Eyeballing it: working with censored data",
    "section": "zero inflated censored Beta regression",
    "text": "zero inflated censored Beta regression\nBuckle up; things are about to escalate quickly.\nWe are finally going to introduce censoring explicitly into our model. Let’s reconsider the data. We are dealing with observations of proportional leaf damage by herbivores, which have been collected in a way that produces censored observations. Continuous proportions bound in the open unit interval (0,1) are easily modeled with a Beta distribution. These data however include several – in fact the majority – of cases with no evidence of damage; they are in the [0,1) interval. A mixture model, zero inflated6 Beta regression is the way to go here. We are modeling the probability of zero with a logistic regression and we are modeling the average proportion for the non-zero cases with a Beta distribution. This is supported in {brms} by specifying family = zero_inflated_beta.\nNext comes the censoring. For the censored observations i.e. all non-zero cases, we need to specify the lower and upper limit of the respective interval. Notice that I have set the lower limit of the first interval slightly above zero (0.01) and the upper limit of the last interval slightly below 1 (0.99). We also need to specify the type of censoring, in this case interval.\n\ndatacens = data %&gt;% \n  mutate(# the lower limits of intervals\n         damagelow = case_when(invert_damage.perc  ==  0 ~ 0,\n                               invert_damage.perc   &lt;  6 ~ .01,\n                               invert_damage.perc   &lt; 26 ~ .06,\n                               invert_damage.perc  &gt;= 26 ~ .26),\n         # the upper limits of intervals\n         damagehigh = case_when(invert_damage.perc  ==  0 ~ 0,\n                                invert_damage.perc   &lt;  6 ~ .05,\n                                invert_damage.perc   &lt; 26 ~ .25,\n                                invert_damage.perc  &gt;= 26 ~ .99),\n         # the type of intervals\n         censor = case_when(invert_damage.perc  ==  0 ~ \"none\",\n                            invert_damage.perc   &lt;  6 ~ \"interval\",\n                            invert_damage.perc   &lt; 26 ~ \"interval\",\n                            invert_damage.perc  &gt;= 26 ~ \"interval\")) \n\nThis is what the {brms} formula for such a model looks like:\n\nformula = bf(\n  damagelow | cens(censor, damagehigh) ~ treat + fgroup + treat:fgroup +\n      (1 | site) + \n      (1 | taxon) + \n      (1 | site:plot:taxon),\n  zi ~ treat + fgroup + treat:fgroup +\n      (1 | site) + \n      (1 | taxon) + \n      (1 | site:plot:taxon)\n)\n\nLower limits are passed to the model in the place of the dependent variable. They are followed by | and then the cens() function that has two terms – the type of censoring and the upper limit for interval censored cases. The other formula is the zero inflation part.\nThe full model would look like this:\n\nS5.1_cens = brm(\n  data = datacens,\n  bf(\n     damagelow | cens(censor, damagehigh) ~ treat + fgroup + treat:fgroup +\n         (1 | site) + \n         (1 | taxon) + \n         (1 | site:plot:taxon),\n     zi ~ treat + fgroup + treat:fgroup +\n         (1 | site) + \n         (1 | taxon) + \n         (1 | site:plot:taxon)\n     ),\n     family = zero_inflated_beta(),\n     iter = 6000,\n     warmup = 2000,\n     chains = 4, \n     cores = 4, \n     seed = 123,\n     backend = \"cmdstanr\"\n)\n\nWe are actually… not going to fit this model! I have tried that and, after the &gt;8 hours it took to finish sampling, it turned out its performance was abysmal. Each chain was doing its own thing (high Rhats) and I could count the Bulk Effective Samples in my fingers. Tinkering with the model (non-flat priors, longer warmup) did not help. Now admittedly, a more seasoned Stan user might have accomplished something better but this looks like a very hard model to fit.\nAccording to the Stan manual, there are two ways to model censored data. One of them “is to treat the censored data as missing data that is constrained to fall in the censored range of values”. I find this is a very intuitive approach, as it maps right onto the description of censored data I gave at the introduction. It is not however, the method currently used by {brms}; instead it is using the second method which “is integrating out censored values”. I do not pretend to understand the second method. I can imagine that not having to impute the censored data can have sampling efficiency benefits under some circumstances but it does not seem to work here…\nWhile digging around in the {brms} github repository, I found a feature request, submitted by Hans Van Calster, for {brms} to provide support for the first method, the data augmentation approach, that imputes the censored values. He also provided Stan code to show what this would look like, following {brms} conventions for writing Stan code.\nOne nice feature of {brms} is that we can use the stancode() function to get the Stan code generated by {brms} for any given model.\n\nstancode(formula, data = datacens, family = zero_inflated_beta())\n\nWe can then modify this code and use it to fit a model directly with Cmdstan. Finally, we can pass the fitted object back into an empty brm object. The benefit of doing this is that any diagnostics, post-processing calculations or plotting we do is within territory familiar to {brms} users.\nIn the code I am showing below, everything except the bits that relate to censoring is as it was generated by {brms}. The censoring bits come from Van Calster’s example. I have highlighted those using //////// borders. I think I’ve put everything in its right place.\n// generated with {brms} 2.22.8 **and modified**\nfunctions {\n  real zero_inflated_beta_lpdf(real y, real mu, real phi, real zi) {\n    row_vector[2] shape = [mu * phi, (1 - mu) * phi];\n    if (y == 0) {\n      return bernoulli_lpmf(1 | zi);\n    } else {\n      return bernoulli_lpmf(0 | zi) +\n             beta_lpdf(y | shape[1], shape[2]);\n    }\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  \n///////////////////////////////////////////////////////////////////////////////////////\n\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  // right censor points for interval censoring\n  vector[N] rcens;\n  \n///////////////////////////////////////////////////////////////////////////////////////\n\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  int&lt;lower=1&gt; K_zi;  // number of population-level effects\n  matrix[N, K_zi] X_zi;  // population-level design matrix\n  int&lt;lower=1&gt; Kc_zi;  // number of population-level effects after centering\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  // data for group-level effects of ID 3\n  int&lt;lower=1&gt; N_3;  // number of grouping levels\n  int&lt;lower=1&gt; M_3;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_3;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_3_1;\n  // data for group-level effects of ID 4\n  int&lt;lower=1&gt; N_4;  // number of grouping levels\n  int&lt;lower=1&gt; M_4;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_4;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_4_zi_1;\n  // data for group-level effects of ID 5\n  int&lt;lower=1&gt; N_5;  // number of grouping levels\n  int&lt;lower=1&gt; M_5;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_5;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_5_zi_1;\n  // data for group-level effects of ID 6\n  int&lt;lower=1&gt; N_6;  // number of grouping levels\n  int&lt;lower=1&gt; M_6;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_6;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_6_zi_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  matrix[N, Kc_zi] Xc_zi;  // centered version of X_zi without an intercept\n  vector[Kc_zi] means_X_zi;  // column means of X_zi before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n  for (i in 2:K_zi) {\n    means_X_zi[i - 1] = mean(X_zi[, i]);\n    Xc_zi[, i - 1] = X_zi[, i] - means_X_zi[i - 1];\n  }\n  \n///////////////////////////////////////////////////////////////////////////////////////\n\n    // censoring indices\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  int Nicens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  array[N] int Jicens;\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    } else if (cens[n] == 2) {\n      Nicens += 1;\n      Jicens[Nicens] = n;\n    }\n  }\n///////////////////////////////////////////////////////////////////////////////////////  \n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; phi;  // precision parameter\n  vector[Kc_zi] b_zi;  // regression coefficients\n  real Intercept_zi;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_3] sd_3;  // group-level standard deviations\n  array[M_3] vector[N_3] z_3;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_4] sd_4;  // group-level standard deviations\n  array[M_4] vector[N_4] z_4;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_5] sd_5;  // group-level standard deviations\n  array[M_5] vector[N_5] z_5;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_6] sd_6;  // group-level standard deviations\n  array[M_6] vector[N_6] z_6;  // standardized group-level effects\n  \n///////////////////////////////////////////////////////////////////////////////////////\n\n    // latent imputed values for censored observations\n  vector&lt;lower=Y[Jrcens[1:Nrcens]], upper=1&gt;[Nrcens] Yright;\n  vector&lt;lower=0, upper=Y[Jlcens[1:Nlcens]]&gt;[Nlcens] Yleft;\n  vector&lt;lower=Y[Jicens[1:Nicens]], upper=rcens[Jicens[1:Nicens]]&gt;[Nicens] Yint;\n  \n///////////////////////////////////////////////////////////////////////////////////////  \n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  vector[N_3] r_3_1;  // actual group-level effects\n  vector[N_4] r_4_zi_1;  // actual group-level effects\n  vector[N_5] r_5_zi_1;  // actual group-level effects\n  vector[N_6] r_6_zi_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  r_3_1 = (sd_3[1] * (z_3[1]));\n  r_4_zi_1 = (sd_4[1] * (z_4[1]));\n  r_5_zi_1 = (sd_5[1] * (z_5[1]));\n  r_6_zi_1 = (sd_6[1] * (z_6[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += gamma_lpdf(phi | 0.01, 0.01);\n  lprior += logistic_lpdf(Intercept_zi | 0, 1);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] zi = rep_vector(0.0, N);\n    mu += Intercept + Xc * b;\n    zi += Intercept_zi + Xc_zi * b_zi;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_3_1[J_3[n]] * Z_3_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      zi[n] += r_4_zi_1[J_4[n]] * Z_4_zi_1[n] + r_5_zi_1[J_5[n]] * Z_5_zi_1[n] + r_6_zi_1[J_6[n]] * Z_6_zi_1[n];\n    }\n    mu = inv_logit(mu);\n    zi = inv_logit(zi);\n    \n///////////////////////////////////////////////////////////////////////////////////////\n\n    // Uncensored data\n    for (i in 1:Nevent) {\n      int n = Jevent[i];\n      target += zero_inflated_beta_lpdf(Y[n] | mu[n], phi, zi[n]);\n    }\n    // Right-censored\n    for (i in 1:Nrcens) {\n      int n = Jrcens[i];\n      target += zero_inflated_beta_lpdf(Yright[i] | mu[n], phi, zi[n]);\n    }\n    // Left-censored\n    for (i in 1:Nlcens) {\n      int n = Jlcens[i];\n      target += zero_inflated_beta_lpdf(Yleft[i] | mu[n], phi, zi[n]);\n    }\n    // Interval-censored\n    for (i in 1: Nicens) {\n      int n = Jicens[i];\n      target += zero_inflated_beta_lpdf(Yint[i] | mu[n], phi, zi[n]);\n    }\n  }\n///////////////////////////////////////////////////////////////////////////////////////  \n\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n  target += std_normal_lpdf(z_3[1]);\n  target += std_normal_lpdf(z_4[1]);\n  target += std_normal_lpdf(z_5[1]);\n  target += std_normal_lpdf(z_6[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n  // actual population-level intercept\n  real b_zi_Intercept = Intercept_zi - dot_product(means_X_zi, b_zi);\n}\nhh\n\nsdata = brms::make_standata(formula, family = zero_inflated_beta(), data = datacens)\n\nmodel = cmdstan_model(\"mcens.stan\")\n\nstanfit = model$sample(\n  data = sdata, \n  chains = 4,  \n  iter_warmup = 2000,  \n  iter_sampling = 4000, \n  seed = 123, \n  parallel_chains = 4\n)\n\n# All 4 chains finished successfully.\n# Mean chain execution time: 2501.8 seconds.\n# Total execution time: 2514.2 seconds.\n\nS5.1_cens = brm(formula, family = zero_inflated_beta(), data = datacens, \n          empty = TRUE)\nS5.1_cens$fit = read_csv_as_stanfit(stanfit$output_files(), model = model)\nS5.1_cens = rename_pars(S5.1_cens)\n\nggg\n\n\nShow the code\np6 = datacens %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;%  \n  add_epred_draws(S5.1_cens, re_formula = NA) %&gt;%\n  ggplot(aes(x = treat, y = .epred, fill = treat)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95)) +\n  scale_fill_manual(values = c(\"#efd08e\",\"#afbedc\",\"#a33c20\", \"#702963\")) +\n  ylab(\"damage proportion\") +\n  xlab(\"nutrient addition treatment\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_grid(. ~ fgroup)\n\np7 = datacens %&gt;%\n  data_grid(treat = factor(c(\"Control\",\"N\",\"P\",\"NP\"), \n                           levels = c(\"Control\",\"N\",\"P\",\"NP\")),\n            fgroup = factor(c(\"GRASS\", \"FORB\", \"LEGUME\"), \n                            levels = c(\"GRASS\", \"FORB\", \"LEGUME\"))) %&gt;% \n  add_epred_draws(S5.1_cens, re_formula = NA) %&gt;%\n  compare_levels(.epred, by = treat,\n                 comparison = control) %&gt;%\n  ggplot(aes(x = .epred, y = treat)) +\n  stat_halfeye(point_interval = \"mean_hdi\",\n               .width = c(0.90, 0.95, .99)) +\n  geom_vline(xintercept = 0, linetype = \"longdash\",\n             color = \"darkred\") +\n  ylab(\"contrast\") +\n  xlab(\"difference\") +\n  facet_wrap(~fgroup) +\n  theme_minimal()\n\n\n\n\n\nFigure 8: Posterior estimates by group and treatment for the zero inflated censored Beta model.\n\n\nhhh\n\n\n\nFigure 9: Posterior estimates for the difference between treatments and control by group for the cumulative probit model.\n\n\nUnfortunately I am not sure how to evaluate this model. Using pp_check() does produce a plot but as it ignores all censored values (there is a warning) it is not informative. I have dealt with cases where it was necessary to produce the density overlay plot manually before but here I am not even sure about what such a plot should look like."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#easy-does-it",
    "href": "posts/2025____-shorttitle/index.html#easy-does-it",
    "title": "Eyeballing it: working with censored data",
    "section": "easy does it?",
    "text": "easy does it?\nA common experience among methods–nerdsoriented people, is that a crude model, which does not quite capture the data generating process, can often yield estimates and lead to conclusions similar to a more sophisticated model that is in principle more well-suited to the task. So why bother? Was all this a bit masturbatory? I think “often” is the crux here; if the crude model gives reliable estimates 99 times out of a 100, without a more appropriate model to compare it to, you don’t know whether your case is one among the 99 or the odd one out. It’s not a binary distinction; some modeling options will be more appropriate than others, some are maybe equally legitimate ways to tackle a problem. But, even though increasing sophistication tends to have diminishing returns, you don’t know the cost of convenience in advance. And if you have taken the trouble of going the extra mile, then you have no reason to fall back to a simpler method..."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#references",
    "href": "posts/2025____-shorttitle/index.html#references",
    "title": "Eyeballing it: working with censored data",
    "section": "references",
    "text": "references\nEbeling, Anne, Alex T. Strauss, Peter B. Adler, Carlos A. Arnillas, Isabel C. Barrio, Lori A. Biederman, Elizabeth T. Borer, et al. 2021. “Nutrient Enrichment Increases Invertebrate Herbivory and Pathogen Damage in Grasslands.” Journal of Ecology, October, 1365-2745.13801. https://doi.org/10.1111/1365-2745.13801."
  },
  {
    "objectID": "posts/2025____-shorttitle/index.html#footnotes",
    "href": "posts/2025____-shorttitle/index.html#footnotes",
    "title": "Eyeballing it: working with censored data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nand pathogen damage but we will leave this aside.↩︎\nand frustratingly, for weirdos like me,↩︎\nThe authors coded the treatments different from what I do here and handled the N:P combination as a statistical interaction. For simplicity, I chose to use control, N, P and NP as four levels in one “treatment” predictor. This will make plotting predictions easier without having a real influence on the estimated differences of interest.↩︎\nA more complex model is worth considering here as, in such a distributed experiment, the treatment effect may vary by site. Similarly, the differences between functional groups may vary by site, given that some sites will not share species, so the functional groups can be represented by different taxa. I prefer to leave all that aside and keep the structure of the model similar to that in the paper.↩︎\ndon’t open that in firefox or the math won’t show properly…↩︎\naugmented if we want to be pedantic↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelos Amyntas",
    "section": "",
    "text": "I am a community ecologist. I am interested in understanding how the assembly of multi-trophic communities changes the functioning of ecosystems in space and across time."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "",
    "section": "",
    "text": "Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB)\nZur alten Fischerhütte 2\n16775\nStechlin\nGermany\nangelos[dot]amyntas[at]posteo[dot]net"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about me",
    "section": "",
    "text": "I recently defended my PhD thesis. In February 2025 I will join Anne McLeod’s Computational Ecology group in IGB Berlin as a postdoc.\nThe last four and a half years, I did research in the context of the Jena Experiment, a longterm grassland biodiversity and ecosystem functioning field experiment, located outside of Jena in Thuringia, Germany. In that phase of the experiment, we were interested in understanding why the positive effect of plant diversity on ecosystem functions — such as productivity — apparently strengthens over time. My project focused on the belowground component of the ecosystem. We examined the effects of plant diversity on functions performed by the multi-trophic community of soil fauna, in old and new communities. This work combined a field experiment, a mesocosm experiment and a bit of modelling.\nMy experiences and readings during this time have shifted my attention towards diversity and functioning relationships at larger spatial scales and under changing environmental conditions. So, to the links that represent trophic relationships between different populations in a community, I am now adding the links that represent interconnected communities in a landscape. Networks everywhere.\n\n\n\nIt’s all connected.\n\n\nI am also increasingly enthusiastic about Bayesian statistics and Agent Based Modelling. For the first, after joining Richard McElreath’s course a couple of years ago, I am teaching myself Stan (although for my work, my needs are usually covered by {brms}). For the second, I am slowly working my way through Paul Smaldino’s Modeling social behavior: Mathematical and agent-based models of social dynamics and cultural evolution, solving the exercises in NetLogo and sometimes trying to do the same using Agents.jl. Why would an ecologist choose a social science textbook to learn ABMs? I like the way he thinks and writes about modelling.\nWhen I am not sciencing, I read novels, listen to noisy music, watch old films or wander about in nature. In order of decreasing frequency."
  },
  {
    "objectID": "posts/20240806/index.html",
    "href": "posts/20240806/index.html",
    "title": "test",
    "section": "",
    "text": "bla\n\n\n\nCitationBibTeX citation:@online{amyntas2024,\n  author = {Amyntas, Angelos},\n  title = {Test},\n  date = {2024-08-06},\n  url = {https://amynang.github.io/posts/20240806/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAmyntas, Angelos. 2024. “Test.” August 6, 2024. https://amynang.github.io/posts/20240806/."
  },
  {
    "objectID": "posts/20250208-diversity-entropy/index.html",
    "href": "posts/20250208-diversity-entropy/index.html",
    "title": "Diversity, entropy and the effective number of species",
    "section": "",
    "text": "note\n\n\n\nThis was originally a short detour in something longer I am working on. I decided to turn it into a standalone short explainer I can point to.\nOne of the metrics that we use to quantify the diversity of an ecological community is Shannon’s entropy. In contrast to “richness”, i.e. the number of species that comprise the community, entropy is determined not only by how many species are there but also their relative abundance. It is calculated as\n\\[\nH' = -\\sum_i p_i log(p_i)\n\\]\nwhere \\(p_i\\) is the relative abundance of species \\(i\\)1. So, if we have an “even” community of four species [🦑, 🐟, 🦀, 🐚] equally abundant, its entropy is:\n-((1/4)*log(1/4))*4\n\n[1] 1.386294\nThere are two ways for entropy to go down. If all 🦑 individuals suddenly vanished2, the new entropy would be:\n-((1/3)*log(1/3))*3\n\n[1] 1.098612\nThe other way for entropy to drop is if the community became less even, changing for example from \\([.25, .25, .25, .25]\\) to \\([.4, .2, .2, .2]\\). Here’s the new entropy:\n-((.4)*log(.4)) - ((.2)*log(.2))*3\n\n[1] 1.332179\nThe number of species determines an upper limit to diversity; how uneven a community is determines how far below that the actual diversity is. The exponent of entropy is sometimes called the effective number of species (for entropy). What does that mean? If we exponentiate the entropy of the initial four-species even community, we get back 4, the number of species. If we exponentiate the uneven community, we get back 3.79. This way, we match our uneven community to an even community of 3.79 species. An even community of 3.79 species has the same entropy as our uneven community, 1.33. So, weird as it may sound, we use \\(species\\) as a continuous unit of measurement to measure diversity in a way that, in fact, incorporates information about both the number of species and the community’s evenness.\n# the entropy of an even 4-species community\n-((1/4)*log(1/4))*4\n\n[1] 1.386294\n\n# the exponent of the entropy of an even 4-species community\nexp(-((1/4)*log(1/4))*4)\n\n[1] 4\n\n# the entropy of an uneven 4-species community with [.4, .2, .2, .2]\n-((.4)*log(.4)) - ((.2)*log(.2))*3\n\n[1] 1.332179\n\n# the exponent of the entropy of an uneven 4-species community with [.4, .2, .2, .2]\nexp(-((.4)*log(.4)) - ((.2)*log(.2))*3)\n\n[1] 3.789291\n\n# the entropy of an even community of 3.79 species\n-((1/3.789291)*log(1/3.789291))*3.789291\n\n[1] 1.332179\nYou can see this from the perspective of a generalist consumer that feeds indiscriminately on these four species. In an environment where the four prey species are equally “available”, the consumer’s diet will consist of four species. In an environment where 97% of prey individuals belong to one species, with the other three species taking up 1% each, the effective number of prey species will be 1.18. The effective diversity of the consumer’s diet is slightly above one species.\nexp(-((.97)*log(.97)) - ((.01)*log(.01))*3)\n\n[1] 1.182582\nWorking with the effective number of species (for Shannon’s entropy), we can examine changes in diversity across an additive scale of even communities. Working with entropy, the natural logarithm of the effective number of species, we examine these changes in a multiplicative scale. One unit change is \\(e\\) times up or down in diversity.\nWell, this is because we have been calculating entropy using the natural logarithm. However, there is no particular reason to use the natural logarithm. Switching to base 2 would make things a bit more intuitive, I think3. When it comes to changes in magnitude, whether it’s time, distance or size, half of something or double of something is more graspable than \\(e\\) times of the thing, right? I would argue that half/double is also more graspable than ten times something or one tenth of the thing4. It’s good to make things as graspable as we can. In fact, one argument in favor of using the effective number of species instead of raw Shannon entropies when communicating changes in diversity, is that we might misrepresent the magnitude of change. I am not sure this is a real issue, as long as we keep in mind that we are dealing with multiplicative changes5. And especially if we work with base 2 logarithms to calculate Shannon entropy, things are fairly straightforward: one unit of change in entropy represents a doubling (or halving) of diversity in terms of the effective number of species."
  },
  {
    "objectID": "posts/20250208-diversity-entropy/index.html#footnotes",
    "href": "posts/20250208-diversity-entropy/index.html#footnotes",
    "title": "Diversity, entropy and the effective number of species",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBased on the number of individuals or, less commonly, biomass. Either makes sense depending on the context, so it is a good idea to be explicit.↩︎\n👽↩︎\n{vegan}’s diversity() has a base argument 😉.↩︎\nIf you were considering the option of base 10. What are you, an astronomer?↩︎\nNever mind that you may exponentiate your entropies because you were encouraged to work with effective numbers of species instead and then you find out you have to log them again for the purposes of your analysis.↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Eyeballing it: working with censored data\n\n\n\nproportional data\n\ncensoring\n\nordinal regression\n\nbrms\n\nStan\n\n\n\nmodelling interval-censored proportional data with brms+\n\n\n\n\n\nJul 12, 2025\n\n\nAngelos Amyntas\n\n\n\n\n\n\n\n\n\n\n\n\nDiversity, entropy and the effective number of species\n\n\n\ndiversity\n\nShannon entropy\n\nlogarithms\n\n\n\n\n\n\n\n\n\nFeb 8, 2025\n\n\nAngelos Amyntas\n\n\n\n\n\n\n\n\n\n\n\n\nRoots, stems & leaves\n\n\n\nproportional data\n\nDirichlet\n\nphylogenetic regression\n\nbrms\n\n\n\nmodelling proportions and trying a phylogenetic regression, using data from 10.1002/ecy.3688\n\n\n\n\n\nSep 7, 2024\n\n\nAngelos Amyntas\n\n\n\n\n\n\n\n\n\n\n\n\ntest\n\n\n\nQuarto\n\n\n\nfiguring out how to write blog posts in quarto\n\n\n\n\n\nAug 6, 2024\n\n\nAngelos Amyntas\n\n\n\n\n\nNo matching items"
  }
]